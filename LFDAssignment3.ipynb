{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, json, argparse\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "numpy.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the NE data, with either 2 or 6 classes\n",
    "def read_corpus(corpus_file, binary_classes):\n",
    "    print('Reading in data from {0}...'.format(corpus_file))\n",
    "    words = []\n",
    "    labels = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            words.append(parts[0])\n",
    "            if binary_classes:\n",
    "                if parts[1] in ['GPE', 'LOC']:\n",
    "                    labels.append('LOCATION')\n",
    "                else:\n",
    "                    labels.append('NON-LOCATION')\n",
    "            else:\n",
    "                labels.append(parts[1])\t\n",
    "    print('Done!')\n",
    "    return words, labels\n",
    "\n",
    "# Read in word embeddings \n",
    "def read_embeddings(embeddings_file):\n",
    "    print('Reading in embeddings from {0}...'.format(embeddings_file))\n",
    "    embeddings = json.load(open(embeddings_file, 'r'))\n",
    "    embeddings = {word:numpy.array(embeddings[word]) for word in embeddings}\n",
    "    print('Done!')\n",
    "    return embeddings\n",
    "\n",
    "# Turn words into embeddings, i.e. replace words by their corresponding embeddings\n",
    "def vectorizer(words, embeddings):\n",
    "    vectorized_words = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            vectorized_words.append(embeddings[word.lower()])\n",
    "        except KeyError:\n",
    "            vectorized_words.append(embeddings['UNK'])\n",
    "    return numpy.array(vectorized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'named_entity_data.txt'\n",
    "embeddings = 'embeddings.json'\n",
    "binary = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in data from named_entity_data.txt...\n",
      "Done!\n",
      "Reading in embeddings from embeddings.json...\n",
      "Done!\n",
      "Epoch 1/1\n",
      "26696/26696 [==============================] - 1s 35us/step - loss: 0.2036 - acc: 0.8516\n",
      "Classification accuracy on test: 0.9177435666928868\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.93      0.89      3092\n",
      "          1       0.96      0.91      0.94      5807\n",
      "\n",
      "avg / total       0.92      0.92      0.92      8899\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the data and embeddings\n",
    "X, Y = read_corpus(data, binary_classes = binary)\n",
    "embeddings = read_embeddings(embeddings)\n",
    "# Transform words to embeddings\n",
    "X = vectorizer(X, embeddings)\n",
    "# Transform string labels to one-hot encodings\n",
    "encoder = LabelBinarizer()\n",
    "Y = encoder.fit_transform(Y) # Use encoder.classes_ to find mapping of one-hot indices to string labels\n",
    "if binary:\n",
    "    Y = numpy.where(Y == 1, [0,1], [1,0])\n",
    "# Split in training and test data\n",
    "split_point = int(0.75*len(X))\n",
    "Xtrain = X[:split_point]\n",
    "Ytrain = Y[:split_point]\n",
    "Xtest = X[split_point:]\n",
    "Ytest = Y[split_point:]\n",
    "# Define the properties of the perceptron model\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim = X.shape[1], units = Y.shape[1]))\n",
    "model.add(Activation(\"linear\"))\n",
    "sgd = SGD(lr = 0.01)\n",
    "loss_function = 'mean_squared_error'\n",
    "model.compile(loss = loss_function, optimizer = sgd, metrics=['accuracy'])\n",
    "# Train the perceptron\n",
    "model.fit(Xtrain, Ytrain, verbose = 1, epochs = 1, batch_size = 32)\n",
    "# Get predictions\n",
    "Yguess = model.predict(Xtest)\n",
    "# Convert to numerical labels to get scores with sklearn in 6-way setting\n",
    "Yguess = numpy.argmax(Yguess, axis = 1)\n",
    "Ytest = numpy.argmax(Ytest, axis = 1)\n",
    "print('Classification accuracy on test: {0}'.format(accuracy_score(Ytest, Yguess)))\n",
    "print('Classification report: ')\n",
    "print(classification_report(Ytest, Yguess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8913 17783]\n",
      "1\n",
      "Classification accuracy on baseline: 0.6525452298011013\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      3092\n",
      "          1       0.65      1.00      0.79      5807\n",
      "\n",
      "avg / total       0.43      0.65      0.52      8899\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/netapps/lwpy/venv-3.5b/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Define a baseline classification by the zero rule algorithm\n",
    "counts = numpy.zeros(len(Ytrain[0]),dtype=int)\n",
    "for i in Ytrain:\n",
    "    idx = numpy.nonzero(i)\n",
    "    counts[idx] = counts[idx] + 1\n",
    "\n",
    "print(counts)\n",
    "most_common = numpy.argmax(counts)\n",
    "print(most_common)\n",
    "\n",
    "Yguess = numpy.full(len(Ytest), most_common)\n",
    "\n",
    "print('Classification accuracy on baseline: {0}'.format(accuracy_score(Ytest, Yguess)))\n",
    "print('Classification report: ')\n",
    "print(classification_report(Ytest, Yguess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in data from named_entity_data.txt...\n",
      "Done!\n",
      "Reading in embeddings from embeddings.json...\n",
      "Done!\n",
      "Epoch 1/1\n",
      "26696/26696 [==============================] - 1s 34us/step - loss: 0.2820 - acc: 0.4987\n",
      "Classification accuracy on test: 0.6580514664569053\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.83      0.76      1311\n",
      "          1       0.65      0.68      0.66      1017\n",
      "          2       0.75      0.82      0.78      2915\n",
      "          3       0.01      0.01      0.01       177\n",
      "          4       0.66      0.37      0.48      2072\n",
      "          5       0.57      0.64      0.61      1407\n",
      "\n",
      "avg / total       0.67      0.66      0.65      8899\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = 'named_entity_data.txt'\n",
    "embeddings = 'embeddings.json'\n",
    "binary = False\n",
    "# Read in the data and embeddings\n",
    "X, Y = read_corpus(data, binary_classes = binary)\n",
    "embeddings = read_embeddings(embeddings)\n",
    "# Transform words to embeddings\n",
    "X = vectorizer(X, embeddings)\n",
    "# Transform string labels to one-hot encodings\n",
    "encoder = LabelBinarizer()\n",
    "Y = encoder.fit_transform(Y) # Use encoder.classes_ to find mapping of one-hot indices to string labels\n",
    "if binary:\n",
    "    Y = numpy.where(Y == 1, [0,1], [1,0])\n",
    "# Split in training and test data\n",
    "split_point = int(0.75*len(X))\n",
    "Xtrain = X[:split_point]\n",
    "Ytrain = Y[:split_point]\n",
    "Xtest = X[split_point:]\n",
    "Ytest = Y[split_point:]\n",
    "# Define the properties of the perceptron model\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim = X.shape[1], units = Y.shape[1]))\n",
    "model.add(Activation(\"linear\"))\n",
    "sgd = SGD(lr = 0.01)\n",
    "loss_function = 'mean_squared_error'\n",
    "model.compile(loss = loss_function, optimizer = sgd, metrics=['accuracy'])\n",
    "# Train the perceptron\n",
    "model.fit(Xtrain, Ytrain, verbose = 1, epochs = 1, batch_size = 32)\n",
    "# Get predictions\n",
    "Yguess = model.predict(Xtest)\n",
    "# Convert to numerical labels to get scores with sklearn in 6-way setting\n",
    "Yguess = numpy.argmax(Yguess, axis = 1)\n",
    "Ytest = numpy.argmax(Ytest, axis = 1)\n",
    "print('Classification accuracy on test: {0}'.format(accuracy_score(Ytest, Yguess)))\n",
    "print('Classification report: ')\n",
    "print(classification_report(Ytest, Yguess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3980 3196 8477  436 6059 4548]\n",
      "2\n",
      "Classification accuracy on baseline: 0.32756489493201485\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      1311\n",
      "          1       0.00      0.00      0.00      1017\n",
      "          2       0.33      1.00      0.49      2915\n",
      "          3       0.00      0.00      0.00       177\n",
      "          4       0.00      0.00      0.00      2072\n",
      "          5       0.00      0.00      0.00      1407\n",
      "\n",
      "avg / total       0.11      0.33      0.16      8899\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/netapps/lwpy/venv-3.5b/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a baseline classification by the zero rule algorithm\n",
    "counts = numpy.zeros(len(Ytrain[0]),dtype=int)\n",
    "for i in Ytrain:\n",
    "    idx = numpy.nonzero(i)\n",
    "    counts[idx] = counts[idx] + 1\n",
    "\n",
    "print(counts)\n",
    "most_common = numpy.argmax(counts)\n",
    "print(most_common)\n",
    "\n",
    "Yguess = numpy.full(len(Ytest), most_common)\n",
    "\n",
    "print('Classification accuracy on baseline: {0}'.format(accuracy_score(Ytest, Yguess)))\n",
    "print('Classification report: ')\n",
    "print(classification_report(Ytest, Yguess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in data from named_entity_data.txt...\n",
      "Done!\n",
      "Reading in embeddings from embeddings.json...\n",
      "Done!\n",
      "Epoch 1/1\n",
      "26696/26696 [==============================] - 1s 32us/step - loss: 0.2152 - acc: 0.8327\n",
      "Epoch 1/2\n",
      "26696/26696 [==============================] - 1s 32us/step - loss: 0.2121 - acc: 0.8375\n",
      "Epoch 2/2\n",
      "26696/26696 [==============================] - 1s 29us/step - loss: 0.0774 - acc: 0.9293\n",
      "Epoch 1/3\n",
      "26696/26696 [==============================] - 1s 35us/step - loss: 0.1781 - acc: 0.8710\n",
      "Epoch 2/3\n",
      "26696/26696 [==============================] - 1s 30us/step - loss: 0.0743 - acc: 0.9338\n",
      "Epoch 3/3\n",
      "26696/26696 [==============================] - 1s 32us/step - loss: 0.0706 - acc: 0.9369\n",
      "Epoch 1/4\n",
      "26696/26696 [==============================] - 1s 34us/step - loss: 0.1865 - acc: 0.8568\n",
      "Epoch 2/4\n",
      "26696/26696 [==============================] - 1s 31us/step - loss: 0.0756 - acc: 0.9332\n",
      "Epoch 3/4\n",
      "26696/26696 [==============================] - 1s 32us/step - loss: 0.0716 - acc: 0.9363\n",
      "Epoch 4/4\n",
      "26696/26696 [==============================] - 1s 31us/step - loss: 0.0711 - acc: 0.9366\n",
      "Epoch 1/5\n",
      "26696/26696 [==============================] - 1s 35us/step - loss: 0.2155 - acc: 0.8499\n",
      "Epoch 2/5\n",
      "26696/26696 [==============================] - 1s 32us/step - loss: 0.0779 - acc: 0.9279\n",
      "Epoch 3/5\n",
      "26696/26696 [==============================] - 1s 30us/step - loss: 0.0709 - acc: 0.9346\n",
      "Epoch 4/5\n",
      "26696/26696 [==============================] - 1s 30us/step - loss: 0.0699 - acc: 0.9369\n",
      "Epoch 5/5\n",
      "26696/26696 [==============================] - 1s 29us/step - loss: 0.0697 - acc: 0.9378\n",
      "Epoch 1/6\n",
      "26696/26696 [==============================] - 1s 34us/step - loss: 0.1811 - acc: 0.8731\n",
      "Epoch 2/6\n",
      "26696/26696 [==============================] - 1s 31us/step - loss: 0.0749 - acc: 0.9353\n",
      "Epoch 3/6\n",
      "26696/26696 [==============================] - 1s 30us/step - loss: 0.0712 - acc: 0.9369\n",
      "Epoch 4/6\n",
      "26696/26696 [==============================] - 1s 31us/step - loss: 0.0706 - acc: 0.9377\n",
      "Epoch 5/6\n",
      "26696/26696 [==============================] - 1s 33us/step - loss: 0.0704 - acc: 0.9380\n",
      "Epoch 6/6\n",
      "26696/26696 [==============================] - 1s 33us/step - loss: 0.0703 - acc: 0.9372\n",
      "Epoch 1/7\n",
      "26696/26696 [==============================] - 1s 37us/step - loss: 0.1883 - acc: 0.8504\n",
      "Epoch 2/7\n",
      "26696/26696 [==============================] - 1s 38us/step - loss: 0.0745 - acc: 0.9311\n",
      "Epoch 3/7\n",
      "26696/26696 [==============================] - 1s 32us/step - loss: 0.0704 - acc: 0.9348\n",
      "Epoch 4/7\n",
      "26696/26696 [==============================] - 1s 31us/step - loss: 0.0698 - acc: 0.9365\n",
      "Epoch 5/7\n",
      "26696/26696 [==============================] - 1s 32us/step - loss: 0.0697 - acc: 0.9364\n",
      "Epoch 6/7\n",
      "26696/26696 [==============================] - 1s 33us/step - loss: 0.0696 - acc: 0.9359\n",
      "Epoch 7/7\n",
      "26696/26696 [==============================] - 1s 31us/step - loss: 0.0696 - acc: 0.9368\n",
      "Epoch 1/8\n",
      "26696/26696 [==============================] - 1s 38us/step - loss: 0.2178 - acc: 0.8632\n",
      "Epoch 2/8\n",
      "26696/26696 [==============================] - 1s 30us/step - loss: 0.0772 - acc: 0.9340\n",
      "Epoch 3/8\n",
      "26696/26696 [==============================] - 1s 32us/step - loss: 0.0709 - acc: 0.9357\n",
      "Epoch 4/8\n",
      "26696/26696 [==============================] - 1s 31us/step - loss: 0.0700 - acc: 0.9360\n",
      "Epoch 5/8\n",
      "26696/26696 [==============================] - 1s 35us/step - loss: 0.0697 - acc: 0.9357\n",
      "Epoch 6/8\n",
      "26696/26696 [==============================] - 1s 32us/step - loss: 0.0696 - acc: 0.9365\n",
      "Epoch 7/8\n",
      "26696/26696 [==============================] - 1s 33us/step - loss: 0.0696 - acc: 0.9364\n",
      "Epoch 8/8\n",
      "26696/26696 [==============================] - 1s 32us/step - loss: 0.0695 - acc: 0.9362\n",
      "Epoch 1/9\n",
      "26696/26696 [==============================] - 1s 37us/step - loss: 0.2031 - acc: 0.8590\n",
      "Epoch 2/9\n",
      "26696/26696 [==============================] - 1s 37us/step - loss: 0.0756 - acc: 0.9338\n",
      "Epoch 3/9\n",
      "26696/26696 [==============================] - 1s 32us/step - loss: 0.0712 - acc: 0.9349\n",
      "Epoch 4/9\n",
      "26696/26696 [==============================] - 1s 32us/step - loss: 0.0706 - acc: 0.9361\n",
      "Epoch 5/9\n",
      "26696/26696 [==============================] - 1s 35us/step - loss: 0.0704 - acc: 0.9357\n",
      "Epoch 6/9\n",
      "26696/26696 [==============================] - 1s 33us/step - loss: 0.0703 - acc: 0.9363\n",
      "Epoch 7/9\n",
      "26696/26696 [==============================] - 1s 36us/step - loss: 0.0702 - acc: 0.9362\n",
      "Epoch 8/9\n",
      "26696/26696 [==============================] - 1s 32us/step - loss: 0.0701 - acc: 0.9357\n",
      "Epoch 9/9\n",
      "26696/26696 [==============================] - 1s 35us/step - loss: 0.0701 - acc: 0.9362\n"
     ]
    }
   ],
   "source": [
    "data = 'named_entity_data.txt'\n",
    "embeddings = 'embeddings.json'\n",
    "binary = True\n",
    "\n",
    "# Read in the data and embeddings\n",
    "X, Y = read_corpus(data, binary_classes = binary)\n",
    "embeddings = read_embeddings(embeddings)\n",
    "# Transform words to embeddings\n",
    "X = vectorizer(X, embeddings)\n",
    "# Transform string labels to one-hot encodings\n",
    "encoder = LabelBinarizer()\n",
    "Y = encoder.fit_transform(Y) # Use encoder.classes_ to find mapping of one-hot indices to string labels\n",
    "if binary:\n",
    "    Y = numpy.where(Y == 1, [0,1], [1,0])\n",
    "\n",
    "epochs = range(1,10)\n",
    "acc = []\n",
    "fscore = []\n",
    "for e in epochs:    \n",
    "    # Split in training and test data\n",
    "    split_point = int(0.75*len(X))\n",
    "    Xtrain = X[:split_point]\n",
    "    Ytrain = Y[:split_point]\n",
    "    Xtest = X[split_point:]\n",
    "    Ytest = Y[split_point:]\n",
    "\n",
    "\n",
    "    # Define the properties of the perceptron model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim = X.shape[1], units = Y.shape[1]))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    sgd = SGD(lr = 0.01)\n",
    "    loss_function = 'mean_squared_error'\n",
    "    model.compile(loss = loss_function, optimizer = sgd, metrics=['accuracy'])\n",
    "\n",
    "    # Train the perceptron\n",
    "    model.fit(Xtrain, Ytrain, verbose = 1, epochs = e, batch_size = 32)\n",
    "    # Get predictions\n",
    "    Yguess = model.predict(Xtest)\n",
    "    # Convert to numerical labels to get scores with sklearn in 6-way setting\n",
    "    Yguess = numpy.argmax(Yguess, axis = 1)\n",
    "    Ytest = numpy.argmax(Ytest, axis = 1)\n",
    "    acc.append(accuracy_score(Ytest, Yguess))\n",
    "    fscore.append(f1_score(Ytest, Yguess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pyplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d3cbc9ac8f5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scores'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pyplot'"
     ]
    }
   ],
   "source": [
    "import pyplot as plt\n",
    "plt.plot(epochs, acc)\n",
    "plt.plot(epochs, fscore)\n",
    "plt.ylabel('scores')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['accuracy', 'f1-score'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
